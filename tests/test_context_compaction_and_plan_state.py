#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Test Suite: Context Compaction & Plan-State Tracking
===================================================================
Multi-Agent Ralph v2.56.0

This test suite validates two CRITICAL systems:

PROBLEM 1: Context Compaction Flow
- PreCompact hook saves state BEFORE compaction
- SessionStart hook restores state AFTER compaction
- additionalContext injection into Claude's context
- Ledger/handoff creation and loading

PROBLEM 2: Plan-State.json Tracking
- Plan-state initialization when orchestrator-analysis.md is written
- Step status updates during implementation
- Progress tracking via statusline
- Plan-sync drift detection
- State-coordinator integration

Test Philosophy: BEHAVIORAL over STATIC
- All tests execute real hooks with real inputs
- Validates actual JSON output format
- Tests edge cases and error conditions

Author: Claude Opus 4.5 (Lead Software Architect)
Version: 2.55.0
Date: 2026-01-20
"""

import json
import os
import subprocess
import tempfile
import time
import shutil
from pathlib import Path
from typing import Any, Dict, Optional
from unittest import TestCase

import pytest


# =============================================================================
# CONFIGURATION
# =============================================================================

HOOKS_DIR = Path.home() / ".claude" / "hooks"
SCRIPTS_DIR = Path.home() / ".claude" / "scripts"
RALPH_DIR = Path.home() / ".ralph"

# Hook paths - Context Compaction
PRE_COMPACT_HOOK = HOOKS_DIR / "pre-compact-handoff.sh"
POST_COMPACT_HOOK = HOOKS_DIR / "post-compact-restore.sh"
SESSION_START_LEDGER_HOOK = HOOKS_DIR / "session-start-ledger.sh"

# Hook paths - Plan State
AUTO_PLAN_STATE_HOOK = HOOKS_DIR / "auto-plan-state.sh"
PLAN_SYNC_POST_STEP_HOOK = HOOKS_DIR / "plan-sync-post-step.sh"
PROGRESS_TRACKER_HOOK = HOOKS_DIR / "progress-tracker.sh"

# Script paths
STATE_COORDINATOR_SCRIPT = SCRIPTS_DIR / "state-coordinator.sh"
STATUSLINE_SCRIPT = Path.home() / ".claude" / "scripts" / "statusline-ralph.sh"

# Timeouts
DEFAULT_TIMEOUT = 10
LONG_TIMEOUT = 30


# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def run_hook(hook_path: Path, input_json: str, cwd: str = None,
             timeout: int = DEFAULT_TIMEOUT, env: Dict[str, str] = None) -> Dict:
    """Execute hook and return comprehensive result."""
    if not hook_path.exists():
        return {
            "returncode": -1,
            "stdout": "",
            "stderr": f"Hook not found: {hook_path}",
            "output": None,
            "is_valid_json": False,
            "execution_time": 0,
        }

    start_time = time.time()
    env_vars = os.environ.copy()
    if env:
        env_vars.update(env)

    try:
        result = subprocess.run(
            ["bash", str(hook_path)],
            input=input_json.encode('utf-8'),
            capture_output=True,
            timeout=timeout,
            cwd=cwd,
            env=env_vars
        )
        execution_time = time.time() - start_time

        stdout = result.stdout.decode('utf-8', errors='replace')
        stderr = result.stderr.decode('utf-8', errors='replace')

        # Try to parse JSON output
        output = None
        is_valid_json = False
        try:
            # Some hooks output multiple lines - get last non-empty line
            lines = [l for l in stdout.strip().split('\n') if l.strip()]
            if lines:
                output = json.loads(lines[-1])
                is_valid_json = True
        except (json.JSONDecodeError, IndexError):
            pass

        return {
            "returncode": result.returncode,
            "stdout": stdout,
            "stderr": stderr,
            "output": output,
            "is_valid_json": is_valid_json,
            "execution_time": execution_time,
        }
    except subprocess.TimeoutExpired:
        return {
            "returncode": -1,
            "stdout": "",
            "stderr": "Timeout expired",
            "output": None,
            "is_valid_json": False,
            "execution_time": timeout,
        }
    except Exception as e:
        return {
            "returncode": -1,
            "stdout": "",
            "stderr": str(e),
            "output": None,
            "is_valid_json": False,
            "execution_time": 0,
        }


def run_script(script_path: Path, args: list = None, cwd: str = None,
               timeout: int = DEFAULT_TIMEOUT) -> Dict:
    """Execute script and return result."""
    if not script_path.exists():
        return {
            "returncode": -1,
            "stdout": "",
            "stderr": f"Script not found: {script_path}",
        }

    cmd = ["bash", str(script_path)]
    if args:
        cmd.extend(args)

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            timeout=timeout,
            cwd=cwd
        )
        return {
            "returncode": result.returncode,
            "stdout": result.stdout.decode('utf-8', errors='replace'),
            "stderr": result.stderr.decode('utf-8', errors='replace'),
        }
    except Exception as e:
        return {
            "returncode": -1,
            "stdout": "",
            "stderr": str(e),
        }


def create_temp_project() -> Path:
    """Create a temporary project directory with .claude structure."""
    temp_dir = Path(tempfile.mkdtemp(prefix="ralph_test_"))
    claude_dir = temp_dir / ".claude"
    claude_dir.mkdir(parents=True)

    # Create minimal CLAUDE.md
    (temp_dir / "CLAUDE.md").write_text("# Test Project\n\nVersion: test\n")

    return temp_dir


def cleanup_temp_project(path: Path):
    """Clean up temporary project directory."""
    if path.exists() and str(path).startswith("/tmp"):
        shutil.rmtree(path, ignore_errors=True)


# =============================================================================
# TEST CLASS: CONTEXT COMPACTION FLOW
# =============================================================================

class TestContextCompactionFlow:
    """
    PROBLEM 1: Validate the complete context compaction flow.

    Flow:
    1. PreCompact hook saves ledger + handoff BEFORE compaction
    2. Context is compacted by Claude Code (simulated)
    3. SessionStart hook (source=compact) restores context
    4. additionalContext is injected into Claude's context

    CRITICAL: The additionalContext MUST be properly formatted and contain
    the ledger/handoff content.
    """

    @pytest.fixture(autouse=True)
    def setup_teardown(self):
        """Setup and teardown for each test."""
        self.temp_project = create_temp_project()
        self.session_id = f"test-session-{int(time.time())}"
        yield
        cleanup_temp_project(self.temp_project)

    # -------------------------------------------------------------------------
    # PreCompact Hook Tests
    # -------------------------------------------------------------------------

    def test_pre_compact_hook_exists_and_executable(self):
        """PreCompact hook must exist and be executable."""
        assert PRE_COMPACT_HOOK.exists(), f"Hook not found: {PRE_COMPACT_HOOK}"
        assert os.access(PRE_COMPACT_HOOK, os.X_OK), "Hook is not executable"

    def test_pre_compact_returns_valid_json(self):
        """PreCompact hook MUST always return valid JSON."""
        input_data = {
            "hook_event_name": "PreCompact",
            "session_id": self.session_id,
            "transcript_path": ""
        }

        result = run_hook(
            PRE_COMPACT_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"], f"Invalid JSON output: {result['stdout']}"
        assert result["output"] is not None

    def test_pre_compact_always_continues(self):
        """PreCompact hooks CANNOT block - must always return continue: true."""
        input_data = {
            "hook_event_name": "PreCompact",
            "session_id": self.session_id,
            "transcript_path": ""
        }

        result = run_hook(
            PRE_COMPACT_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["output"] is not None
        assert result["output"].get("continue") is True, \
            f"PreCompact must return continue: true, got: {result['output']}"

    def test_pre_compact_creates_ledger(self):
        """PreCompact hook should create a ledger file."""
        input_data = {
            "hook_event_name": "PreCompact",
            "session_id": self.session_id,
            "transcript_path": ""
        }

        result = run_hook(
            PRE_COMPACT_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        # Check ledger was created
        ledger_path = RALPH_DIR / "ledgers" / f"CONTINUITY_RALPH-{self.session_id}.md"

        # Note: The hook may fail to create ledger if ledger-manager.py is not available
        # This test documents expected behavior
        if not ledger_path.exists():
            pytest.skip("Ledger not created - ledger-manager.py may not be available")

        assert ledger_path.exists(), f"Ledger not created: {ledger_path}"
        content = ledger_path.read_text()
        assert "CONTINUITY_RALPH" in content or "CURRENT GOAL" in content

    def test_pre_compact_creates_handoff(self):
        """PreCompact hook should create a handoff file."""
        input_data = {
            "hook_event_name": "PreCompact",
            "session_id": self.session_id,
            "transcript_path": ""
        }

        result = run_hook(
            PRE_COMPACT_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        # Check handoff directory
        handoff_dir = RALPH_DIR / "handoffs" / self.session_id

        # Note: The hook may fail if handoff-generator.py is not available
        if not handoff_dir.exists():
            pytest.skip("Handoff not created - handoff-generator.py may not be available")

        handoff_files = list(handoff_dir.glob("handoff-*.md"))
        assert len(handoff_files) > 0, f"No handoff files in {handoff_dir}"

    def test_pre_compact_handles_empty_input(self):
        """PreCompact hook should handle empty/invalid input gracefully."""
        test_cases = [
            "{}",
            '{"session_id": "test"}',
            "invalid json",
            "",
        ]

        for input_data in test_cases:
            result = run_hook(
                PRE_COMPACT_HOOK,
                input_data,
                cwd=str(self.temp_project)
            )

            # Should not crash
            assert result["returncode"] == 0 or result["is_valid_json"], \
                f"Hook crashed on input: {input_data}"

    # -------------------------------------------------------------------------
    # SessionStart Hook (Post-Compact) Tests
    # -------------------------------------------------------------------------

    def test_session_start_ledger_hook_exists(self):
        """SessionStart ledger hook must exist and be executable."""
        assert SESSION_START_LEDGER_HOOK.exists(), f"Hook not found: {SESSION_START_LEDGER_HOOK}"
        assert os.access(SESSION_START_LEDGER_HOOK, os.X_OK), "Hook is not executable"

    def test_session_start_returns_valid_json(self):
        """SessionStart hook MUST return valid JSON with hookSpecificOutput."""
        input_data = {
            "hook_event_name": "SessionStart",
            "session_id": self.session_id,
            "source": "compact"
        }

        result = run_hook(
            SESSION_START_LEDGER_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"], f"Invalid JSON output: {result['stdout']}"

    def test_session_start_has_hook_specific_output(self):
        """SessionStart hook MUST return hookSpecificOutput with additionalContext."""
        input_data = {
            "hook_event_name": "SessionStart",
            "session_id": self.session_id,
            "source": "compact"
        }

        result = run_hook(
            SESSION_START_LEDGER_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["output"] is not None
        assert "hookSpecificOutput" in result["output"], \
            f"Missing hookSpecificOutput: {result['output']}"

        hook_output = result["output"]["hookSpecificOutput"]
        assert "hookEventName" in hook_output, "Missing hookEventName"
        assert hook_output["hookEventName"] == "SessionStart"
        assert "additionalContext" in hook_output, \
            "CRITICAL: additionalContext missing - context will NOT be injected"

    def test_session_start_additional_context_format(self):
        """additionalContext MUST be a string that can be injected into context."""
        input_data = {
            "hook_event_name": "SessionStart",
            "session_id": self.session_id,
            "source": "compact"
        }

        result = run_hook(
            SESSION_START_LEDGER_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        if result["output"] and "hookSpecificOutput" in result["output"]:
            additional_context = result["output"]["hookSpecificOutput"].get("additionalContext", "")

            # Must be a string
            assert isinstance(additional_context, str), \
                f"additionalContext must be string, got: {type(additional_context)}"

    def test_session_start_loads_ledger_content(self):
        """SessionStart should include ledger content in additionalContext."""
        # First, create a ledger
        ledger_path = RALPH_DIR / "ledgers" / f"CONTINUITY_RALPH-{self.session_id}.md"
        ledger_path.parent.mkdir(parents=True, exist_ok=True)
        ledger_path.write_text("""# CONTINUITY_RALPH: test-session
## CURRENT GOAL
Testing ledger restoration

## COMPLETED WORK
- [x] Test task completed
""")

        try:
            input_data = {
                "hook_event_name": "SessionStart",
                "session_id": self.session_id,
                "source": "compact"
            }

            result = run_hook(
                SESSION_START_LEDGER_HOOK,
                json.dumps(input_data),
                cwd=str(self.temp_project)
            )

            if result["output"] and "hookSpecificOutput" in result["output"]:
                additional_context = result["output"]["hookSpecificOutput"].get("additionalContext", "")

                # Should contain ledger content markers
                # Note: May be empty if ledger loading is disabled
                if additional_context:
                    assert "Session Ledger" in additional_context or "CONTINUITY" in additional_context, \
                        f"Ledger content not found in context: {additional_context[:200]}"
        finally:
            # Cleanup
            if ledger_path.exists():
                ledger_path.unlink()

    def test_post_compact_restore_hook_format(self):
        """Post-compact restore hook must return proper SessionStart format."""
        input_data = {
            "hook_event_name": "SessionStart",
            "session_id": self.session_id,
            "source": "compact"
        }

        result = run_hook(
            POST_COMPACT_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"], f"Invalid JSON: {result['stdout']}"

        if result["output"]:
            assert "hookSpecificOutput" in result["output"], \
                "post-compact-restore must return hookSpecificOutput"

    # -------------------------------------------------------------------------
    # Full Flow Integration Test
    # -------------------------------------------------------------------------

    def test_full_compaction_flow(self):
        """
        INTEGRATION TEST: Complete compaction flow.

        Simulates:
        1. Session active with state
        2. PreCompact triggered (saves state)
        3. Compaction happens (context cleared)
        4. SessionStart (source=compact) triggered (restores state)
        5. Verify context is restored
        """
        # Step 1: Create session state (progress.md)
        progress_file = self.temp_project / ".claude" / "progress.md"
        progress_file.write_text("""# Progress Log

## Current Goal
Testing compaction flow

## Recent Progress
- Step 1: Created test files
- Step 2: Modified configuration
""")

        # Step 2: Trigger PreCompact
        pre_compact_input = {
            "hook_event_name": "PreCompact",
            "session_id": self.session_id,
            "transcript_path": ""
        }

        pre_result = run_hook(
            PRE_COMPACT_HOOK,
            json.dumps(pre_compact_input),
            cwd=str(self.temp_project)
        )

        assert pre_result["returncode"] == 0, f"PreCompact failed: {pre_result['stderr']}"

        # Step 3: Simulate compaction (just verify pre-compact completed)
        assert pre_result["output"].get("continue") is True

        # Step 4: Trigger SessionStart (source=compact)
        session_start_input = {
            "hook_event_name": "SessionStart",
            "session_id": self.session_id,
            "source": "compact"
        }

        session_result = run_hook(
            SESSION_START_LEDGER_HOOK,
            json.dumps(session_start_input),
            cwd=str(self.temp_project)
        )

        assert session_result["is_valid_json"], \
            f"SessionStart returned invalid JSON: {session_result['stdout']}"

        # Step 5: Verify context structure
        if session_result["output"] and "hookSpecificOutput" in session_result["output"]:
            hook_output = session_result["output"]["hookSpecificOutput"]
            assert "additionalContext" in hook_output, \
                "CRITICAL: additionalContext not present - context restoration FAILED"


# =============================================================================
# TEST CLASS: PLAN-STATE TRACKING
# =============================================================================

class TestPlanStateTracking:
    """
    PROBLEM 2: Validate plan-state.json tracking and updates.

    Flow:
    1. Orchestrator writes orchestrator-analysis.md
    2. auto-plan-state.sh hook creates plan-state.json
    3. During implementation, steps are updated
    4. plan-sync-post-step.sh detects drift
    5. Statusline displays progress accurately
    """

    @pytest.fixture(autouse=True)
    def setup_teardown(self):
        """Setup and teardown for each test."""
        self.temp_project = create_temp_project()
        yield
        cleanup_temp_project(self.temp_project)

    # -------------------------------------------------------------------------
    # Auto-Plan-State Hook Tests
    # -------------------------------------------------------------------------

    def test_auto_plan_state_hook_exists(self):
        """auto-plan-state.sh hook must exist and be executable."""
        assert AUTO_PLAN_STATE_HOOK.exists(), f"Hook not found: {AUTO_PLAN_STATE_HOOK}"
        assert os.access(AUTO_PLAN_STATE_HOOK, os.X_OK), "Hook is not executable"

    def test_auto_plan_state_only_triggers_on_analysis(self):
        """Hook should only trigger when orchestrator-analysis.md is written."""
        # Test with non-matching file
        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Write",
            "tool_input": {
                "file_path": str(self.temp_project / "some-other-file.txt")
            }
        }

        result = run_hook(
            AUTO_PLAN_STATE_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"]
        assert result["output"].get("continue") is True

        # Plan-state should NOT be created
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        assert not plan_state.exists(), "Plan-state should not be created for non-matching files"

    def test_auto_plan_state_creates_plan_on_analysis(self):
        """Hook should create plan-state.json when orchestrator-analysis.md is written."""
        # Create orchestrator-analysis.md
        analysis_file = self.temp_project / ".claude" / "orchestrator-analysis.md"
        analysis_file.write_text("""# Orchestrator Analysis
Task: Test Task Implementation

## Classification
- **Complexity**: 7/10
- **Model Routing**: opus
- **Adversarial Required**: Yes

## Implementation Plan

### Phase 1: Setup
1. Create directory structure
2. Initialize configuration

### Phase 2: Implementation
3. Implement core logic
4. Add error handling

### Phase 3: Testing
5. Write unit tests
6. Integration tests
""")

        # Trigger hook
        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Write",
            "tool_input": {
                "file_path": str(analysis_file)
            }
        }

        result = run_hook(
            AUTO_PLAN_STATE_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"], f"Invalid JSON: {result['stdout']}"

        # Check plan-state was created
        plan_state = self.temp_project / ".claude" / "plan-state.json"

        if not plan_state.exists():
            pytest.skip("Plan-state not created - state-coordinator may not be available")

        content = json.loads(plan_state.read_text())

        assert "plan_id" in content, "Missing plan_id"
        assert "task" in content, "Missing task"
        assert "classification" in content, "Missing classification"
        assert content["classification"]["complexity"] == 7

    def test_auto_plan_state_extracts_steps(self):
        """Hook should extract steps from orchestrator-analysis.md."""
        # Create analysis with clear step markers
        analysis_file = self.temp_project / ".claude" / "orchestrator-analysis.md"
        analysis_file.write_text("""# Analysis
Task: Extract Steps Test
Complexity: 5

### Step 1: First Step
Do something

### Step 2: Second Step
Do something else

### Step 3: Third Step
Final thing
""")

        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Write",
            "tool_input": {
                "file_path": str(analysis_file)
            }
        }

        result = run_hook(
            AUTO_PLAN_STATE_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        plan_state = self.temp_project / ".claude" / "plan-state.json"

        if not plan_state.exists():
            pytest.skip("Plan-state not created")

        content = json.loads(plan_state.read_text())
        steps = content.get("steps", {})

        # Should have extracted at least some steps
        assert len(steps) > 0, "No steps extracted from analysis"

    def test_auto_plan_state_returns_continue(self):
        """Hook should return continue: true after processing."""
        analysis_file = self.temp_project / ".claude" / "orchestrator-analysis.md"
        analysis_file.write_text("# Test Analysis\nTask: Test\nComplexity: 3")

        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Write",
            "tool_input": {
                "file_path": str(analysis_file)
            }
        }

        result = run_hook(
            AUTO_PLAN_STATE_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["output"].get("continue") is True, \
            f"Hook should return continue: true, got: {result['output']}"

    # -------------------------------------------------------------------------
    # Plan-Sync Post-Step Hook Tests
    # -------------------------------------------------------------------------

    def test_plan_sync_hook_exists(self):
        """plan-sync-post-step.sh hook must exist."""
        assert PLAN_SYNC_POST_STEP_HOOK.exists(), f"Hook not found: {PLAN_SYNC_POST_STEP_HOOK}"

    def test_plan_sync_skips_without_plan_state(self):
        """Hook should skip when no plan-state.json exists."""
        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Write",
            "tool_input": {
                "file_path": str(self.temp_project / "test.ts")
            }
        }

        result = run_hook(
            PLAN_SYNC_POST_STEP_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"]
        assert result["output"].get("continue") is True

    def test_plan_sync_requires_file_path(self):
        """Hook should require CLAUDE_TOOL_ARG_file_path environment variable."""
        # Create plan-state
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        plan_state.write_text(json.dumps({
            "plan_id": "test",
            "task": "Test",
            "steps": []
        }))

        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Write",
            "tool_input": {}
        }

        result = run_hook(
            PLAN_SYNC_POST_STEP_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        # Should skip without file_path
        assert result["is_valid_json"]
        assert result["output"].get("continue") is True

    def test_plan_sync_detects_drift(self):
        """Hook should detect drift when actual exports differ from spec."""
        # Create plan-state with expected exports
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        plan_state.write_text(json.dumps({
            "plan_id": "test",
            "task": "Test",
            "steps": [
                {
                    "id": "1",
                    "name": "Create service",
                    "status": "in_progress",
                    "spec": {
                        "file": "src/service.ts",
                        "exports": ["authenticate", "logout"],
                        "signatures": {}
                    },
                    "actual": None,
                    "drift": {
                        "detected": False,
                        "items": [],
                        "needs_sync": False
                    }
                }
            ]
        }))

        # Create the file with DIFFERENT exports
        src_dir = self.temp_project / "src"
        src_dir.mkdir()
        service_file = src_dir / "service.ts"
        service_file.write_text("""
export function login() {}
export function logout() {}
""")

        # Set environment variable for file path
        env = {
            "CLAUDE_TOOL_ARG_file_path": str(service_file)
        }

        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Write",
            "tool_input": {
                "file_path": str(service_file)
            }
        }

        result = run_hook(
            PLAN_SYNC_POST_STEP_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project),
            env=env
        )

        # Should complete successfully
        assert result["is_valid_json"]

        # Check if drift was detected (may require file to match step)
        # Note: Drift detection depends on file path matching step spec

    # -------------------------------------------------------------------------
    # Progress Tracker Hook Tests
    # -------------------------------------------------------------------------

    def test_progress_tracker_hook_exists(self):
        """progress-tracker.sh hook must exist."""
        assert PROGRESS_TRACKER_HOOK.exists(), f"Hook not found: {PROGRESS_TRACKER_HOOK}"

    def test_progress_tracker_only_tracks_relevant_tools(self):
        """Hook should only track Edit, Write, Bash tools."""
        # Test with irrelevant tool
        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Read",
            "session_id": "test-session"
        }

        result = run_hook(
            PROGRESS_TRACKER_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"]
        assert result["output"].get("continue") is True

        # Progress file should NOT be created for Read
        progress_file = self.temp_project / ".claude" / "progress.md"
        # May or may not exist depending on previous tests

    def test_progress_tracker_creates_progress_file(self):
        """Hook should create progress.md for Write/Edit/Bash tools."""
        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Write",
            "session_id": "test-session",
            "tool_input": {
                "file_path": str(self.temp_project / "test.txt")
            },
            "tool_result": "Success"
        }

        result = run_hook(
            PROGRESS_TRACKER_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"]

        # Progress file should be created
        progress_file = self.temp_project / ".claude" / "progress.md"
        assert progress_file.exists(), "Progress file not created"

        content = progress_file.read_text()
        assert "Progress Log" in content or "Write:" in content

    def test_progress_tracker_records_errors(self):
        """Hook should record errors in progress.md."""
        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Bash",
            "session_id": "test-session",
            "tool_input": {
                "command": "failing-command"
            },
            "tool_result": "Error: command not found"
        }

        result = run_hook(
            PROGRESS_TRACKER_HOOK,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"]

        progress_file = self.temp_project / ".claude" / "progress.md"
        if progress_file.exists():
            content = progress_file.read_text()
            # Should contain error indicator
            assert "Error" in content or ":x:" in content, \
                "Error not recorded in progress"

    # -------------------------------------------------------------------------
    # State Coordinator Tests
    # -------------------------------------------------------------------------

    def test_state_coordinator_exists(self):
        """state-coordinator.sh must exist and be executable."""
        assert STATE_COORDINATOR_SCRIPT.exists(), f"Script not found: {STATE_COORDINATOR_SCRIPT}"
        assert os.access(STATE_COORDINATOR_SCRIPT, os.X_OK), "Script is not executable"

    def test_state_coordinator_init(self):
        """state-coordinator init should create valid plan-state.json."""
        result = run_script(
            STATE_COORDINATOR_SCRIPT,
            ["init", "Test Task", "7", "opus", "STANDARD"],
            cwd=str(self.temp_project)
        )

        # Should return plan_id
        assert result["returncode"] == 0, f"Init failed: {result['stderr']}"

        # The script outputs event_id on first line and plan_id on second line
        lines = result["stdout"].strip().split("\n")
        plan_id = lines[-1] if lines else ""
        assert plan_id, "No plan_id returned"

        # Check plan-state was created
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        assert plan_state.exists(), "Plan-state not created by init"

        content = json.loads(plan_state.read_text())
        assert content["plan_id"] == plan_id
        assert content["task"] == "Test Task"
        assert content["classification"]["complexity"] == 7

    def test_state_coordinator_add_phase(self):
        """state-coordinator add-phase should add phases correctly."""
        # First init
        run_script(
            STATE_COORDINATOR_SCRIPT,
            ["init", "Test", "5", "sonnet"],
            cwd=str(self.temp_project)
        )

        # Add phase
        result = run_script(
            STATE_COORDINATOR_SCRIPT,
            ["add-phase", "clarify", "Clarification", "sequential"],
            cwd=str(self.temp_project)
        )

        assert result["returncode"] == 0, f"Add-phase failed: {result['stderr']}"

        # Verify phase added
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        content = json.loads(plan_state.read_text())

        phases = content.get("phases", [])
        assert len(phases) > 0, "No phases added"
        assert phases[0]["phase_id"] == "clarify"

    def test_state_coordinator_add_step(self):
        """state-coordinator add-step should add steps to phases."""
        # Init and add phase
        run_script(
            STATE_COORDINATOR_SCRIPT,
            ["init", "Test", "5"],
            cwd=str(self.temp_project)
        )
        run_script(
            STATE_COORDINATOR_SCRIPT,
            ["add-phase", "impl", "Implementation", "sequential"],
            cwd=str(self.temp_project)
        )

        # Add step
        result = run_script(
            STATE_COORDINATOR_SCRIPT,
            ["add-step", "step1", "First Step", "impl"],
            cwd=str(self.temp_project)
        )

        assert result["returncode"] == 0, f"Add-step failed: {result['stderr']}"

        # Verify step added
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        content = json.loads(plan_state.read_text())

        steps = content.get("steps", {})
        assert "step1" in steps, "Step not added"
        assert steps["step1"]["name"] == "First Step"

    def test_state_coordinator_update_step_status(self):
        """state-coordinator update-step should change step status."""
        # Setup
        run_script(STATE_COORDINATOR_SCRIPT, ["init", "Test", "5"], cwd=str(self.temp_project))
        run_script(STATE_COORDINATOR_SCRIPT, ["add-phase", "impl", "Impl", "sequential"], cwd=str(self.temp_project))
        run_script(STATE_COORDINATOR_SCRIPT, ["add-step", "s1", "Step 1", "impl"], cwd=str(self.temp_project))

        # Update to in_progress
        result = run_script(
            STATE_COORDINATOR_SCRIPT,
            ["update-step", "s1", "in_progress"],
            cwd=str(self.temp_project)
        )

        assert result["returncode"] == 0

        # Verify status changed
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        content = json.loads(plan_state.read_text())
        assert content["steps"]["s1"]["status"] == "in_progress"

        # Update to completed
        run_script(
            STATE_COORDINATOR_SCRIPT,
            ["update-step", "s1", "completed", "success"],
            cwd=str(self.temp_project)
        )

        content = json.loads(plan_state.read_text())
        assert content["steps"]["s1"]["status"] == "completed"
        assert content["steps"]["s1"]["result"] == "success"

    def test_state_coordinator_status_output(self):
        """state-coordinator status should show current state."""
        # Setup some state
        run_script(STATE_COORDINATOR_SCRIPT, ["init", "Status Test", "6"], cwd=str(self.temp_project))
        run_script(STATE_COORDINATOR_SCRIPT, ["add-phase", "p1", "Phase 1"], cwd=str(self.temp_project))
        run_script(STATE_COORDINATOR_SCRIPT, ["add-step", "s1", "Step 1", "p1"], cwd=str(self.temp_project))
        run_script(STATE_COORDINATOR_SCRIPT, ["update-step", "s1", "completed"], cwd=str(self.temp_project))

        # Get status
        result = run_script(
            STATE_COORDINATOR_SCRIPT,
            ["status"],
            cwd=str(self.temp_project)
        )

        assert result["returncode"] == 0
        assert "Status Test" in result["stdout"] or "ORCHESTRATION" in result["stdout"]

    def test_state_coordinator_status_compact(self):
        """state-coordinator status --compact should return one-liner."""
        run_script(STATE_COORDINATOR_SCRIPT, ["init", "Test", "5"], cwd=str(self.temp_project))
        run_script(STATE_COORDINATOR_SCRIPT, ["add-phase", "p1", "P1"], cwd=str(self.temp_project))
        run_script(STATE_COORDINATOR_SCRIPT, ["add-step", "s1", "S1", "p1"], cwd=str(self.temp_project))

        result = run_script(
            STATE_COORDINATOR_SCRIPT,
            ["status", "compact"],
            cwd=str(self.temp_project)
        )

        assert result["returncode"] == 0
        # Should be single line
        lines = [l for l in result["stdout"].strip().split('\n') if l.strip()]
        assert len(lines) == 1, f"Compact status should be one line: {result['stdout']}"

    # -------------------------------------------------------------------------
    # Statusline Tests
    # -------------------------------------------------------------------------

    def test_statusline_script_exists(self):
        """statusline-ralph.sh must exist."""
        statusline = Path.home() / ".claude" / "scripts" / "statusline-ralph.sh"
        assert statusline.exists(), f"Statusline script not found: {statusline}"

    def test_statusline_reads_plan_state(self):
        """Statusline should read and display plan-state progress."""
        # Create plan-state with some progress
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        plan_state.write_text(json.dumps({
            "version": "2.54.0",
            "plan_id": "test",
            "task": "Test",
            "classification": {
                "route": "STANDARD"
            },
            "steps": {
                "1": {"status": "completed"},
                "2": {"status": "completed"},
                "3": {"status": "in_progress"},
                "4": {"status": "pending"},
                "5": {"status": "pending"}
            }
        }))

        # Run statusline
        statusline = Path.home() / ".claude" / "scripts" / "statusline-ralph.sh"
        result = subprocess.run(
            ["bash", str(statusline)],
            input=json.dumps({"cwd": str(self.temp_project)}).encode(),
            capture_output=True,
            timeout=5,
            cwd=str(self.temp_project)
        )

        output = result.stdout.decode('utf-8', errors='replace')

        # Should show progress (2/5 = 40%)
        # Format: ðŸ“Š 2/5 40% or similar
        assert "2" in output or "40" in output, \
            f"Progress not shown in statusline: {output}"


# =============================================================================
# TEST CLASS: INTEGRATION - END-TO-END
# =============================================================================

class TestEndToEndIntegration:
    """
    Integration tests for the complete workflow.

    Tests the interaction between:
    - Context compaction hooks
    - Plan-state tracking
    - State coordinator
    - Progress tracking
    """

    @pytest.fixture(autouse=True)
    def setup_teardown(self):
        """Setup and teardown."""
        self.temp_project = create_temp_project()
        self.session_id = f"integration-{int(time.time())}"
        yield
        cleanup_temp_project(self.temp_project)

    def test_full_orchestration_cycle(self):
        """
        Test complete orchestration cycle:
        1. Initialize plan
        2. Add phases and steps
        3. Execute steps with progress tracking
        4. Simulate compaction mid-cycle
        5. Restore context
        6. Continue execution
        7. Verify final state
        """
        # Step 1: Initialize plan
        result = run_script(
            STATE_COORDINATOR_SCRIPT,
            ["init", "Full Cycle Test", "7", "opus"],
            cwd=str(self.temp_project)
        )
        assert result["returncode"] == 0
        # The script outputs event_id on first line and plan_id on second line
        lines = result["stdout"].strip().split("\n")
        plan_id = lines[-1] if lines else ""

        # Step 2: Add phases
        run_script(
            STATE_COORDINATOR_SCRIPT,
            ["add-phase", "clarify", "Clarification", "sequential"],
            cwd=str(self.temp_project)
        )
        run_script(
            STATE_COORDINATOR_SCRIPT,
            ["add-phase", "implement", "Implementation", "parallel"],
            cwd=str(self.temp_project)
        )

        # Add steps
        run_script(STATE_COORDINATOR_SCRIPT, ["add-step", "s1", "Gather requirements", "clarify"], cwd=str(self.temp_project))
        run_script(STATE_COORDINATOR_SCRIPT, ["add-step", "s2", "Implement core", "implement"], cwd=str(self.temp_project))
        run_script(STATE_COORDINATOR_SCRIPT, ["add-step", "s3", "Implement tests", "implement"], cwd=str(self.temp_project))

        # Step 3: Execute first step
        run_script(STATE_COORDINATOR_SCRIPT, ["update-step", "s1", "in_progress"], cwd=str(self.temp_project))
        run_script(STATE_COORDINATOR_SCRIPT, ["update-step", "s1", "completed", "success"], cwd=str(self.temp_project))

        # Verify progress
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        content = json.loads(plan_state.read_text())
        assert content["steps"]["s1"]["status"] == "completed"

        # Step 4: Simulate PreCompact
        pre_compact_input = {
            "hook_event_name": "PreCompact",
            "session_id": self.session_id
        }
        result = run_hook(
            PRE_COMPACT_HOOK,
            json.dumps(pre_compact_input),
            cwd=str(self.temp_project)
        )
        assert result["output"].get("continue") is True

        # Step 5: Simulate SessionStart (restore)
        session_input = {
            "hook_event_name": "SessionStart",
            "session_id": self.session_id,
            "source": "compact"
        }
        result = run_hook(
            SESSION_START_LEDGER_HOOK,
            json.dumps(session_input),
            cwd=str(self.temp_project)
        )
        assert result["is_valid_json"]

        # Step 6: Continue execution
        run_script(STATE_COORDINATOR_SCRIPT, ["update-step", "s2", "in_progress"], cwd=str(self.temp_project))
        run_script(STATE_COORDINATOR_SCRIPT, ["update-step", "s2", "completed", "success"], cwd=str(self.temp_project))

        # Step 7: Verify final state
        content = json.loads(plan_state.read_text())
        assert content["steps"]["s1"]["status"] == "completed"
        assert content["steps"]["s2"]["status"] == "completed"
        assert content["steps"]["s3"]["status"] == "pending"

        # Check status
        result = run_script(
            STATE_COORDINATOR_SCRIPT,
            ["status", "compact"],
            cwd=str(self.temp_project)
        )
        assert "2" in result["stdout"], "Progress should show 2 completed"

    def test_plan_survives_implementation(self):
        """
        CRITICAL TEST: Plan must survive implementation.

        This validates that:
        1. Plan is created with spec
        2. Implementation proceeds
        3. Drift is detected
        4. Plan is updated (not replaced)
        5. Downstream steps are patched
        """
        # Create orchestrator analysis
        analysis = self.temp_project / ".claude" / "orchestrator-analysis.md"
        analysis.write_text("""# Analysis
Task: Plan Survival Test
Complexity: 6

### Phase 1: Setup
1. Create auth service

### Phase 2: Implementation
2. Add login endpoint
3. Add logout endpoint
""")

        # Trigger auto-plan-state
        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Write",
            "tool_input": {"file_path": str(analysis)}
        }
        run_hook(AUTO_PLAN_STATE_HOOK, json.dumps(input_data), cwd=str(self.temp_project))

        # Verify plan was created
        plan_state = self.temp_project / ".claude" / "plan-state.json"

        if not plan_state.exists():
            pytest.skip("Plan-state not created")

        initial_content = json.loads(plan_state.read_text())
        initial_task = initial_content["task"]

        # Simulate some implementation
        (self.temp_project / "src").mkdir()
        (self.temp_project / "src" / "auth.ts").write_text("export function login() {}")

        # Trigger progress tracker
        input_data = {
            "hook_event_name": "PostToolUse",
            "tool_name": "Write",
            "session_id": "test",
            "tool_input": {"file_path": str(self.temp_project / "src" / "auth.ts")}
        }
        run_hook(PROGRESS_TRACKER_HOOK, json.dumps(input_data), cwd=str(self.temp_project))

        # Verify plan STILL EXISTS and is not corrupted
        assert plan_state.exists(), "Plan was deleted during implementation!"

        final_content = json.loads(plan_state.read_text())
        assert final_content["task"] == initial_task, "Plan task was changed!"
        assert final_content["plan_id"] == initial_content["plan_id"], "Plan ID changed!"


# =============================================================================
# TEST CLASS: PLAN-STATE LIFECYCLE (v2.56.0)
# =============================================================================

class TestPlanStateLifecycle:
    """
    v2.56.0: Test plan-state-lifecycle.sh auto-archive functionality.

    Key behaviors:
    1. Stale plans (>2 hours) should be auto-archived when new task detected
    2. /orchestrator command should always archive existing plan
    3. Archive should preserve plan to ~/.ralph/archive/plans/
    """

    @pytest.fixture(autouse=True)
    def setup_teardown(self):
        """Setup and teardown for each test."""
        self.temp_project = create_temp_project()
        self.archive_dir = RALPH_DIR / "archive" / "plans"
        self.archive_dir.mkdir(parents=True, exist_ok=True)
        yield
        cleanup_temp_project(self.temp_project)

    def test_plan_state_lifecycle_hook_exists(self):
        """plan-state-lifecycle.sh must exist and be executable."""
        hook_path = HOOKS_DIR / "plan-state-lifecycle.sh"
        assert hook_path.exists(), f"Hook not found: {hook_path}"
        assert os.access(hook_path, os.X_OK), "Hook is not executable"

    def test_lifecycle_skips_without_plan_state(self):
        """Hook should skip when no plan-state.json exists."""
        hook_path = HOOKS_DIR / "plan-state-lifecycle.sh"

        input_data = {
            "userPromptContent": "Implement a new feature with authentication"
        }

        result = run_hook(
            hook_path,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"]
        assert result["output"] == {}, "Should return empty JSON when no plan exists"

    def test_lifecycle_detects_stale_plan_with_new_task(self):
        """Hook should detect stale plan when new task prompt is submitted."""
        hook_path = HOOKS_DIR / "plan-state-lifecycle.sh"

        # Create an old plan-state
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        plan_state.write_text(json.dumps({
            "plan_id": "old-plan",
            "task": "Old Task from Previous Session",
            "steps": {
                "1": {"status": "completed"},
                "2": {"status": "pending"}
            }
        }))

        # Make file old (2+ hours) - simulate by touching with old timestamp
        import time
        old_time = time.time() - (3 * 3600)  # 3 hours ago
        os.utime(str(plan_state), (old_time, old_time))

        # Submit a new task prompt
        input_data = {
            "userPromptContent": "Implement a new OAuth authentication system with JWT tokens"
        }

        result = run_hook(
            hook_path,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"]

        # Should modify prompt with archive notice or warning
        if result["output"].get("userPromptContent"):
            content = result["output"]["userPromptContent"]
            assert "PLAN-STATE" in content, "Should include plan-state notice"

    def test_lifecycle_auto_archive_creates_archive_file(self):
        """Hook should create archive file when auto-archiving."""
        hook_path = HOOKS_DIR / "plan-state-lifecycle.sh"

        # Create an old plan-state
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        plan_state.write_text(json.dumps({
            "plan_id": "archive-test-plan",
            "task": "Task to be Archived",
            "steps": {"1": {"status": "completed"}}
        }))

        # Make file old
        old_time = time.time() - (3 * 3600)
        os.utime(str(plan_state), (old_time, old_time))

        # Count archives before
        archives_before = list(self.archive_dir.glob("plan-*.json"))

        # Submit new task
        input_data = {
            "userPromptContent": "Create a new microservice architecture with Docker"
        }

        env = {"PLAN_STATE_AUTO_ARCHIVE": "true"}
        result = run_hook(
            hook_path,
            json.dumps(input_data),
            cwd=str(self.temp_project),
            env=env
        )

        # Check if auto-archive occurred
        if "AUTO-ARCHIVED" in result.get("output", {}).get("userPromptContent", ""):
            # Plan should be removed
            assert not plan_state.exists(), "Plan should be removed after archive"

            # New archive should exist
            archives_after = list(self.archive_dir.glob("plan-*.json"))
            assert len(archives_after) > len(archives_before), "Archive file should be created"

    def test_lifecycle_does_not_archive_recent_plan(self):
        """Hook should NOT archive recent plans (<2 hours old)."""
        hook_path = HOOKS_DIR / "plan-state-lifecycle.sh"

        # Create a recent plan-state
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        plan_state.write_text(json.dumps({
            "plan_id": "recent-plan",
            "task": "Recent Active Task",
            "steps": {"1": {"status": "in_progress"}}
        }))

        # Don't modify the timestamp - it's recent

        input_data = {
            "userPromptContent": "Implement new feature X"
        }

        result = run_hook(
            hook_path,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        # Plan should NOT be archived
        assert plan_state.exists(), "Recent plan should NOT be archived"

    def test_lifecycle_detects_orchestrator_command(self):
        """Hook should archive any existing plan when /orchestrator is invoked."""
        hook_path = HOOKS_DIR / "plan-state-lifecycle.sh"

        # Create a recent plan
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        plan_state.write_text(json.dumps({
            "plan_id": "existing-plan",
            "task": "Existing Task",
            "steps": {}
        }))

        # Invoke /orchestrator
        input_data = {
            "userPromptContent": "/orchestrator Implement new database layer"
        }

        env = {"PLAN_STATE_AUTO_ARCHIVE": "true"}
        result = run_hook(
            hook_path,
            json.dumps(input_data),
            cwd=str(self.temp_project),
            env=env
        )

        # The hook runs successfully
        assert result["is_valid_json"]

    def test_lifecycle_handles_continuation_prompts(self):
        """Hook should NOT archive when prompt indicates continuation."""
        hook_path = HOOKS_DIR / "plan-state-lifecycle.sh"

        # Create old plan
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        plan_state.write_text(json.dumps({
            "plan_id": "continue-plan",
            "task": "Ongoing Task",
            "steps": {"1": {"status": "in_progress"}}
        }))

        old_time = time.time() - (3 * 3600)
        os.utime(str(plan_state), (old_time, old_time))

        # Continuation prompt
        input_data = {
            "userPromptContent": "Continue with the previous task and fix the remaining issues"
        }

        result = run_hook(
            hook_path,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        # Should NOT trigger archive for continuation
        # (heuristic: contains "continue", "fix", "complete", etc.)
        assert result["output"] == {} or "AUTO-ARCHIVED" not in result["output"].get("userPromptContent", "")


# =============================================================================
# TEST CLASS: TODO-PLAN-SYNC (v2.56.0)
# =============================================================================

class TestTodoPlanSync:
    """
    v2.56.0: Test todo-plan-sync.sh functionality.

    Note: TodoWrite is NOT a valid PostToolUse matcher in Claude Code.
    These tests verify the hook logic works when invoked directly.
    """

    @pytest.fixture(autouse=True)
    def setup_teardown(self):
        """Setup and teardown."""
        self.temp_project = create_temp_project()
        yield
        cleanup_temp_project(self.temp_project)

    def test_todo_plan_sync_hook_exists(self):
        """todo-plan-sync.sh must exist and be executable."""
        hook_path = HOOKS_DIR / "todo-plan-sync.sh"
        assert hook_path.exists(), f"Hook not found: {hook_path}"
        assert os.access(hook_path, os.X_OK), "Hook is not executable"

    def test_todo_sync_ignores_non_todowrite(self):
        """Hook should ignore non-TodoWrite tools."""
        hook_path = HOOKS_DIR / "todo-plan-sync.sh"

        input_data = {
            "tool_name": "Write",
            "tool_input": {"file_path": "/some/file.txt"}
        }

        result = run_hook(
            hook_path,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"]
        assert result["output"].get("continue") is True

    def test_todo_sync_creates_plan_from_todos(self):
        """Hook should create plan-state from todos when invoked directly."""
        hook_path = HOOKS_DIR / "todo-plan-sync.sh"

        # Simulate TodoWrite input
        input_data = {
            "tool_name": "TodoWrite",
            "tool_input": {
                "todos": [
                    {"content": "Analyze requirements", "status": "completed", "activeForm": "Analyzing"},
                    {"content": "Implement feature", "status": "in_progress", "activeForm": "Implementing"},
                    {"content": "Write tests", "status": "pending", "activeForm": "Writing tests"}
                ]
            }
        }

        result = run_hook(
            hook_path,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"]
        assert result["output"].get("continue") is True

        # Check plan-state was created
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        if plan_state.exists():
            content = json.loads(plan_state.read_text())
            assert "steps" in content
            # Should have 3 steps matching todos
            assert len(content["steps"]) == 3

    def test_todo_sync_updates_existing_plan(self):
        """Hook should update existing plan-state with todo progress."""
        hook_path = HOOKS_DIR / "todo-plan-sync.sh"

        # Create existing plan-state
        plan_state = self.temp_project / ".claude" / "plan-state.json"
        plan_state.write_text(json.dumps({
            "plan_id": "existing",
            "task": "Existing Task",
            "steps": {
                "1": {"name": "Step 1", "status": "pending"},
                "2": {"name": "Step 2", "status": "pending"},
                "3": {"name": "Step 3", "status": "pending"}
            }
        }))

        # Update with completed todos
        input_data = {
            "tool_name": "TodoWrite",
            "tool_input": {
                "todos": [
                    {"content": "Step 1", "status": "completed", "activeForm": "Step 1"},
                    {"content": "Step 2", "status": "in_progress", "activeForm": "Step 2"},
                    {"content": "Step 3", "status": "pending", "activeForm": "Step 3"}
                ]
            }
        }

        result = run_hook(
            hook_path,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"]

        # Verify steps were updated
        content = json.loads(plan_state.read_text())
        assert content["steps"]["1"]["status"] == "completed"
        assert content["steps"]["2"]["status"] == "in_progress"
        assert content["steps"]["3"]["status"] == "pending"

    def test_todo_sync_handles_empty_todos(self):
        """Hook should handle empty todo list gracefully."""
        hook_path = HOOKS_DIR / "todo-plan-sync.sh"

        input_data = {
            "tool_name": "TodoWrite",
            "tool_input": {
                "todos": []
            }
        }

        result = run_hook(
            hook_path,
            json.dumps(input_data),
            cwd=str(self.temp_project)
        )

        assert result["is_valid_json"]
        assert result["output"].get("continue") is True


# =============================================================================
# MAIN
# =============================================================================

if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
