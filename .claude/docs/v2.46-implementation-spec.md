# Implementation Specification: Ralph Orchestrator v2.46

> **Version**: 2.46.0-spec
> **Status**: DRAFT
> **Based on**: analysis-v2.46-proposal.md

---

## 1. Enhanced Complexity Classification

### 1.1 New Classification Schema

```yaml
# File: ~/.claude/schemas/task-classification-v2.yaml

TaskClassification:
  # Existing dimension (enhanced)
  complexity:
    type: integer
    range: [1, 10]
    description: "Task complexity score"
    thresholds:
      trivial: [1, 2]      # Single-line fixes
      simple: [3, 4]        # Single file, clear scope
      moderate: [5, 6]      # Multi-file, some decisions
      complex: [7, 8]       # Architectural, many files
      critical: [9, 10]     # Security, payments, auth

  # NEW dimension from RLM paper
  information_density:
    type: enum
    values:
      - CONSTANT    # Answer size fixed regardless of input (O(1))
      - LINEAR      # Answer scales with input size (O(N))
      - QUADRATIC   # Answer scales with input^2 (O(N^2))
    examples:
      CONSTANT: "Find specific function", "Fix typo", "Look up config"
      LINEAR: "Summarize each file", "Lint all files", "Migrate all endpoints"
      QUADRATIC: "Find all dependencies", "Cross-reference modules", "Pair analysis"

  # NEW dimension
  context_requirement:
    type: enum
    values:
      - FITS        # Task fits in single context window (<100k tokens)
      - CHUNKED     # Needs chunking but sequential processing OK
      - RECURSIVE   # Needs recursive decomposition with aggregation
    assessment:
      FITS: "Total relevant context < 100k tokens"
      CHUNKED: "Context 100k-500k tokens, can process sequentially"
      RECURSIVE: "Context >500k tokens OR needs cross-chunk reasoning"

  # Decision output
  workflow_route:
    type: enum
    values:
      - FAST_PATH          # Skip orchestration, direct execute
      - STANDARD           # Full 12-step orchestration
      - PARALLEL_CHUNKS    # Parallel chunk processing
      - RECURSIVE_DECOMPOSE # Recursive sub-orchestrators
```

### 1.2 Classification Decision Matrix

```python
# File: ~/.claude/hooks/classify-task.py

def determine_workflow_route(classification):
    """
    Decision matrix for workflow routing based on RLM paper insights.

    Priority: Quality > Consistency (as requested)
    """
    c = classification['complexity']
    d = classification['information_density']
    r = classification['context_requirement']

    # FAST_PATH: Trivial tasks that don't need orchestration
    if c <= 3 and d == 'CONSTANT' and r == 'FITS':
        return 'FAST_PATH'

    # RECURSIVE_DECOMPOSE: Complex tasks needing recursive processing
    if r == 'RECURSIVE' or d == 'QUADRATIC':
        return 'RECURSIVE_DECOMPOSE'

    # PARALLEL_CHUNKS: Large but not quadratic complexity
    if r == 'CHUNKED' or d == 'LINEAR':
        return 'PARALLEL_CHUNKS'

    # STANDARD: Everything else
    return 'STANDARD'
```

---

## 2. Fast-Path Implementation

### 2.1 Fast-Path Flow

```yaml
# Step 0: EVALUATE with Fast-Path decision

EVALUATE:
  description: "Quick assessment to determine if full orchestration needed"

  inputs:
    - user_request: string
    - project_context: object  # From SessionStart

  process:
    1. Quick complexity scan:
       - Single file mentioned? → likely trivial
       - Keywords: "quick", "fix", "typo", "simple" → likely trivial
       - Keywords: "implement", "design", "migrate" → non-trivial

    2. Information density assessment:
       - "all files", "each module" → LINEAR/QUADRATIC
       - "this function", "that line" → CONSTANT

    3. Context requirement check:
       - File count < 5 → FITS
       - File count 5-20 → CHUNKED
       - File count > 20 OR cross-repo → RECURSIVE

  outputs:
    classification:
      complexity: integer
      information_density: enum
      context_requirement: enum
      workflow_route: enum

  fast_path_criteria:
    ALL of:
      - complexity <= 3
      - information_density == CONSTANT
      - context_requirement == FITS
      - no_architectural_impact == true
      - user_confirms_simple == true (implicit or explicit)
```

### 2.2 Fast-Path Execution

```yaml
FAST_PATH_FLOW:
  description: "3-step flow for trivial tasks"

  steps:
    1. DIRECT_EXECUTE:
       - No planning phase
       - Direct implementation
       - Single subagent if needed

    2. MICRO_VALIDATE:
       - Run affected linters only
       - Type check modified files only
       - No adversarial validation

    3. DONE:
       - No retrospective (too trivial)
       - Log completion for metrics

  max_duration: "5 minutes"
  max_iterations: 3
```

---

## 3. Parallel Exploration Implementation

### 3.1 Step 1c: PARALLEL_EXPLORE

```yaml
# New step after CLARIFY, before CLASSIFY

PARALLEL_EXPLORE:
  description: "Launch parallel exploration tasks to gather context efficiently"

  trigger: "After GAP_ANALYST completes, before CLASSIFY"

  parallel_tasks:
    semantic_search:
      tool: "tldr semantic"
      args: ["$extracted_keywords", "."]
      timeout: 30s
      purpose: "Find related code semantically"

    file_structure:
      tool: "tldr structure"
      args: ["."]
      timeout: 15s
      purpose: "Get codebase structure summary"

    dependency_scan:
      tool: "tldr deps"
      args: ["$primary_file", "."]
      timeout: 20s
      purpose: "Understand dependencies"

    pattern_search:
      tool: "ast-grep"
      args: ["$identified_patterns"]
      timeout: 30s
      purpose: "Find similar implementations"

    web_research:
      tool: "mcp__MiniMax__web_search"
      args: ["$research_query"]
      timeout: 45s
      condition: "Only if external reference needed"
      purpose: "Gather external context"

  aggregation:
    output_file: ".claude/exploration-context.json"
    schema:
      relevant_files: array[string]
      existing_patterns: array[object]
      dependencies: object
      external_references: array[object]
      exploration_timestamp: datetime

  error_handling:
    on_timeout: "Use partial results"
    on_failure: "Log and continue without that data"
```

### 3.2 Implementation Hook

```bash
#!/bin/bash
# File: ~/.claude/hooks/parallel-explore.sh
# Hook: PostToolUse (after AskUserQuestion in CLARIFY)
# Trigger: When gap-analyst completes

set -euo pipefail
umask 077

# Parse input
INPUT=$(cat)
TOOL_NAME=$(echo "$INPUT" | jq -r '.tool_name // empty')
SESSION_ID=$(echo "$INPUT" | jq -r '.session_id // "unknown"')

# Only trigger after GAP_ANALYST
if [[ "$TOOL_NAME" != "Task" ]]; then
    echo '{"decision": "continue"}'
    exit 0
fi

# Check if this was a gap-analyst task
TASK_TYPE=$(echo "$INPUT" | jq -r '.tool_input.subagent_type // empty')
if [[ "$TASK_TYPE" != "gap-analyst" ]]; then
    echo '{"decision": "continue"}'
    exit 0
fi

# Extract keywords from clarification
KEYWORDS=$(cat .claude/orchestrator-analysis.md 2>/dev/null | grep -A5 "Keywords:" | head -5 || echo "")

# Launch parallel exploration
LOG_FILE="$HOME/.ralph/logs/parallel-explore-$(date +%Y%m%d-%H%M%S).log"

{
    echo "[$(date)] Starting parallel exploration"

    # Task 1: Semantic search
    (tldr semantic "$KEYWORDS" . 2>/dev/null || echo "[]") > /tmp/explore-semantic.json &
    PID1=$!

    # Task 2: Structure
    (tldr structure . 2>/dev/null || echo "{}") > /tmp/explore-structure.json &
    PID2=$!

    # Task 3: Pattern search
    (ast-grep --pattern "$KEYWORDS" --json 2>/dev/null || echo "[]") > /tmp/explore-patterns.json &
    PID3=$!

    # Wait for all with timeout
    timeout 60 wait $PID1 $PID2 $PID3 || true

    # Aggregate results
    jq -n \
        --slurpfile semantic /tmp/explore-semantic.json \
        --slurpfile structure /tmp/explore-structure.json \
        --slurpfile patterns /tmp/explore-patterns.json \
        '{
            relevant_files: ($semantic[0] // []),
            structure: ($structure[0] // {}),
            patterns: ($patterns[0] // []),
            timestamp: now | todate
        }' > .claude/exploration-context.json

    echo "[$(date)] Parallel exploration complete"
} >> "$LOG_FILE" 2>&1 &

# Return immediately, exploration runs in background
echo '{"decision": "continue", "exploration_started": true}'
```

---

## 4. Recursive Decomposition Implementation

### 4.1 Step 3d: RECURSIVE_DECOMPOSE

```yaml
RECURSIVE_DECOMPOSE:
  description: "Decompose complex tasks into sub-orchestrations"

  trigger: "When workflow_route == RECURSIVE_DECOMPOSE"

  process:
    1. Identify logical chunks:
       - By module/package
       - By feature area
       - By file group
       - By dependency graph

    2. For each chunk, create sub-plan:
       - Extract relevant requirements
       - Define verifiable specs
       - Set boundaries (what's in/out of scope)

    3. Spawn sub-orchestrators:
       - Each with isolated context
       - Each runs mini 8-step flow
       - Max depth: 3 (prevents infinite recursion)

    4. Monitor and aggregate:
       - Track sub-orchestrator progress
       - Collect sub-results
       - Reconcile conflicts

  sub_orchestrator_config:
    max_depth: 3
    inherit:
      - worktree (if active)
      - quality_gates
      - model_routing
    isolate:
      - plan_state (separate instance)
      - context (fresh per sub)

  aggregation_strategy:
    interfaces: "Merge, resolve conflicts"
    implementations: "Keep separate, ensure compatibility"
    tests: "Combine into test suite"
    documentation: "Merge with dedup"
```

### 4.2 Sub-Orchestrator Spawning

```yaml
# Task configuration for sub-orchestrator

Task:
  subagent_type: "orchestrator"
  model: "sonnet"  # Sub-orchestrators use Sonnet for efficiency
  run_in_background: true
  prompt: |
    SUB_ORCHESTRATION_CONTEXT:
      parent_task_id: "{parent_id}"
      depth: {current_depth + 1}
      max_depth: 3
      chunk_id: "{chunk_id}"
      chunk_scope: |
        {chunk_description}

      inherited_constraints:
        - worktree: {worktree_path}
        - quality_gates: enabled
        - adversarial: {if depth==1 and complexity >= 7}

      verifiable_specs:
        {specs_for_this_chunk}

      boundaries:
        in_scope: {in_scope_items}
        out_of_scope: {out_of_scope_items}

    INSTRUCTION:
      Execute mini-orchestration for this chunk.
      Use STANDARD flow (8 steps).
      Report completion via plan-state update.

      DO NOT:
      - Modify files outside your chunk scope
      - Spawn additional sub-orchestrators (you're at depth {depth})
      - Change shared interfaces without flagging
```

### 4.3 Plan-State Schema Extension

```json
{
  "$schema": "plan-state-v2",
  "plan_id": "uuid",
  "task": "description",
  "classification": {
    "complexity": 8,
    "information_density": "QUADRATIC",
    "context_requirement": "RECURSIVE",
    "workflow_route": "RECURSIVE_DECOMPOSE"
  },
  "recursion": {
    "depth": 0,
    "max_depth": 3,
    "parent_id": null,
    "children": [
      {
        "chunk_id": "oauth-google",
        "sub_plan_id": "uuid-1",
        "status": "in_progress",
        "depth": 1
      },
      {
        "chunk_id": "oauth-github",
        "sub_plan_id": "uuid-2",
        "status": "pending",
        "depth": 1
      }
    ]
  },
  "steps": [...],
  "loop_state": {...}
}
```

---

## 5. Parallel Execution Enhancement

### 5.1 Enhanced Step 6: EXECUTE-WITH-SYNC

```yaml
EXECUTE_WITH_SYNC_V2:
  description: "Execute steps with parallel substep support"

  for_each_step:
    1. Analyze step for parallelization:
       - Identify independent substeps
       - Check for data dependencies
       - Determine safe parallel groups

    2. Execute parallel groups:
       parallel_groups = find_parallel_groups(step.substeps)

       for group in parallel_groups:
         if len(group) > 1:
           # Parallel execution
           tasks = [spawn_subagent(s, run_in_background=True) for s in group]
           results = await_all(tasks, timeout=300)
           reconcile(results)
         else:
           # Sequential execution
           execute(group[0])

    3. Post-step validation:
       - LSA-VERIFY
       - PLAN-SYNC (detect drift)
       - MICRO-GATE (3-fix rule)

  parallel_group_rules:
    safe_to_parallelize:
      - Independent files (no imports between them)
      - Test files (can run in parallel)
      - Documentation (isolated)

    must_be_sequential:
      - Interface definition → Implementation
      - Schema → Migration → Seed
      - Core module → Dependent modules
```

### 5.2 Dependency Analysis

```python
# File: ~/.claude/hooks/analyze-dependencies.py

def find_parallel_groups(substeps):
    """
    Analyze substeps and group them for parallel execution.

    From RLM paper: "Alternative strategies involving asynchronous
    sub-calls can significantly reduce runtime."
    """
    # Build dependency graph
    graph = {}
    for step in substeps:
        deps = extract_dependencies(step)
        graph[step.id] = deps

    # Find independent groups using topological sort
    groups = []
    remaining = set(graph.keys())

    while remaining:
        # Find all steps with no unmet dependencies
        ready = {s for s in remaining
                 if all(d not in remaining for d in graph[s])}

        if not ready:
            # Circular dependency - fall back to sequential
            groups.append(list(remaining))
            break

        groups.append(list(ready))
        remaining -= ready

    return groups
```

---

## 6. Quality-First Validation

### 6.1 Reordered Validation Stages

```yaml
VALIDATE_V2:
  description: "Quality-first validation (quality over consistency)"

  stages:
    # Stage 1: CORRECTNESS (blocking)
    correctness:
      priority: 1
      blocking: true
      checks:
        - requirements_met: "All stated requirements implemented"
        - edge_cases: "Edge cases handled"
        - functional: "Feature works as specified"
        - complete: "No TODO/FIXME left unresolved"

      on_fail: "Return to EXECUTE with specific issues"

    # Stage 2: QUALITY (blocking)
    quality:
      priority: 2
      blocking: true
      checks:
        - security: "No vulnerabilities introduced"
        - performance: "No obvious performance issues"
        - error_handling: "Errors handled gracefully"
        - tests: "Adequate test coverage"

      on_fail: "Return to EXECUTE with quality issues"

    # Stage 3: CONSISTENCY (advisory - NOT blocking)
    consistency:
      priority: 3
      blocking: false  # KEY CHANGE: consistency doesn't block
      checks:
        - patterns: "Follows codebase patterns"
        - style: "Matches code style"
        - naming: "Consistent naming conventions"
        - structure: "File organization matches project"

      on_fail: "Log warning, continue to next stage"
      rationale: |
        From user requirement: "priorizando la calidad sobre la consistencia"
        Consistency issues are noted but don't prevent shipping.

    # Stage 4: ADVERSARIAL (if complexity >= 7)
    adversarial:
      priority: 4
      blocking: true
      condition: "classification.complexity >= 7"
      checks:
        - dual_model: "Claude Opus + Codex agree"
        - coverage: "100% spec coverage verified"

      on_fail: "Return to EXECUTE with adversarial findings"
```

### 6.2 Updated Quality Gates Hook

```bash
#!/bin/bash
# File: ~/.claude/hooks/quality-gates-v2.sh
# Updated for quality-first validation

set -euo pipefail

# Parse input
INPUT=$(cat)
FILE_PATH=$(echo "$INPUT" | jq -r '.tool_input.file_path // empty')

# Skip if no file
[[ -z "$FILE_PATH" ]] && echo '{"decision": "continue"}' && exit 0

# Determine file type
EXT="${FILE_PATH##*.}"

# Run appropriate checks
ERRORS=""
WARNINGS=""

case "$EXT" in
    py)
        # QUALITY (blocking)
        if ! python -m py_compile "$FILE_PATH" 2>&1; then
            ERRORS+="Syntax error\n"
        fi
        if ! mypy "$FILE_PATH" --ignore-missing-imports 2>&1 | grep -q "error:"; then
            : # OK
        else
            ERRORS+="Type errors\n"
        fi

        # CONSISTENCY (advisory - warnings only)
        if ! ruff check "$FILE_PATH" 2>&1; then
            WARNINGS+="Style issues (ruff)\n"
        fi
        ;;

    ts|tsx)
        # QUALITY (blocking)
        if ! npx tsc --noEmit "$FILE_PATH" 2>&1; then
            ERRORS+="TypeScript errors\n"
        fi

        # CONSISTENCY (advisory)
        if ! npx eslint "$FILE_PATH" 2>&1; then
            WARNINGS+="ESLint warnings\n"
        fi
        ;;
esac

# Prepare response
if [[ -n "$ERRORS" ]]; then
    # Blocking issues - fail
    echo "{\"decision\": \"block\", \"errors\": \"$ERRORS\", \"warnings\": \"$WARNINGS\"}"
else
    # Only warnings (consistency) - continue with note
    if [[ -n "$WARNINGS" ]]; then
        echo "{\"decision\": \"continue\", \"warnings\": \"$WARNINGS\", \"note\": \"Consistency issues noted but not blocking (quality over consistency)\"}"
    else
        echo "{\"decision\": \"continue\"}"
    fi
fi
```

---

## 7. Updated Orchestrator Skill

### 7.1 New SKILL.md Structure

```yaml
---
# VERSION: 2.46.0
name: orchestrator
description: "Full orchestration with RLM-inspired improvements: fast-path for trivial tasks, parallel exploration, recursive decomposition, and quality-first validation."
context: fork
user-invocable: true
agent: orchestrator
allowed-tools:
  - Task
  - AskUserQuestion
  - EnterPlanMode
  - ExitPlanMode
  - TodoWrite
  - Read
  - Edit
  - Write
  - Bash
  - Glob
  - Grep
hooks:
  SessionStart:
    - path: ~/.claude/hooks/orchestrator-init.sh
      once: true
  PreToolUse:
    - path: ~/.claude/hooks/fast-path-check.sh
      tools: ["Task"]
  PostToolUse:
    - path: ~/.claude/hooks/parallel-explore.sh
      tools: ["Task"]
    - path: ~/.claude/hooks/quality-gates-v2.sh
      tools: ["Edit", "Write"]
  Stop:
    - path: ~/.claude/hooks/orchestrator-report.sh
---

# Orchestrator - Multi-Agent Ralph v2.46

## Core Workflow (Updated)

### Step 0: EVALUATE (NEW - Mandatory)

**CRITICAL**: Assess task BEFORE full orchestration.

Fast-Path Criteria (ALL must be true):
- complexity <= 3
- information_density == CONSTANT
- context_requirement == FITS
- No architectural impact
- User confirms simplicity (implicit or explicit)

If FAST_PATH:
  → DIRECT_EXECUTE → MICRO_VALIDATE → DONE (3 steps)

If not FAST_PATH:
  → Continue to Step 1 (full orchestration)

### Step 1: CLARIFY (existing)
### Step 1b: GAP-ANALYST (existing)

### Step 1c: PARALLEL_EXPLORE (NEW)

Launch in parallel:
- semantic_search: Find related code
- file_structure: Get codebase overview
- dependency_scan: Understand dependencies
- pattern_search: Find similar implementations
- web_research: External context (if needed)

Wait for all, aggregate to .claude/exploration-context.json

### Step 2: CLASSIFY (Enhanced)

Now includes THREE dimensions:
1. Task Complexity (1-10) - existing
2. Information Density (CONSTANT/LINEAR/QUADRATIC) - NEW
3. Context Requirement (FITS/CHUNKED/RECURSIVE) - NEW

Decision Matrix determines workflow_route:
- FAST_PATH: Already handled in Step 0
- STANDARD: Normal 12-step flow
- PARALLEL_CHUNKS: Parallel processing of chunks
- RECURSIVE_DECOMPOSE: Spawn sub-orchestrators

### Step 2b: WORKTREE (existing)
### Step 3: PLAN (existing)
### Step 3b: PERSIST (existing)
### Step 3c: PLAN-STATE (existing)

### Step 3d: RECURSIVE_DECOMPOSE (NEW)

If workflow_route == RECURSIVE_DECOMPOSE:
1. Identify logical chunks
2. Create sub-plan per chunk
3. Spawn sub-orchestrators (max depth: 3)
4. Aggregate results

### Step 4: PLAN MODE (existing)
### Step 5: DELEGATE (existing)

### Step 6: EXECUTE-WITH-SYNC (Enhanced)

Now with parallel substep support:
1. Analyze step for parallelization
2. Group independent substeps
3. Execute groups in parallel
4. Reconcile results
5. LSA-VERIFY → PLAN-SYNC → MICRO-GATE

### Step 7: VALIDATE (Reordered - Quality First)

1. CORRECTNESS (blocking) - Meets requirements?
2. QUALITY (blocking) - Security, performance?
3. CONSISTENCY (advisory) - Style, patterns? (NOT blocking)
4. ADVERSARIAL (if complexity >= 7)

**KEY**: Consistency issues logged but don't block. Quality over consistency.

### Step 8: RETROSPECT (existing)

→ VERIFIED_DONE
```

---

## 8. Migration Guide

### 8.1 From v2.45 to v2.46

```bash
# 1. Update global skill
cp ~/.claude/skills/orchestrator/SKILL.md ~/.claude/skills/orchestrator/SKILL.md.v2.45.bak
# Apply new SKILL.md from this spec

# 2. Add new hooks
cp parallel-explore.sh ~/.claude/hooks/
cp fast-path-check.sh ~/.claude/hooks/
cp quality-gates-v2.sh ~/.claude/hooks/

# 3. Update plan-state schema
# Add new fields: information_density, context_requirement, recursion

# 4. Test with sample tasks
ralph orch "Fix typo in README"        # Should use FAST_PATH
ralph orch "Implement OAuth for all providers"  # Should use RECURSIVE_DECOMPOSE
```

### 8.2 Backwards Compatibility

- All v2.45 features preserved
- New features are additive
- Fast-path is opt-in (user can force full orchestration)
- Recursive decomposition only triggers when needed

---

## 9. Success Metrics

| Metric | v2.45 Baseline | v2.46 Target |
|--------|----------------|--------------|
| Trivial task time | 5-10 min | 1-2 min |
| Complex task success | 70% | 85% |
| Plan survival rate | 80% | 95% |
| Token usage | 100% | 70% |
| Parallel speedup | 1x | 3x |

---

## 10. Open Questions

1. **Recursion depth**: Is 3 levels sufficient? RLM paper uses depth=1.
2. **Parallel timeout**: 60s for exploration - enough?
3. **Consistency as advisory**: Will users accept style issues?
4. **Fast-path criteria**: Should user explicitly opt-in?

---

## Appendix: File Changes Summary

| File | Change | Priority |
|------|--------|----------|
| `~/.claude/skills/orchestrator/SKILL.md` | Complete rewrite | P0 |
| `~/.claude/hooks/fast-path-check.sh` | NEW | P0 |
| `~/.claude/hooks/parallel-explore.sh` | NEW | P1 |
| `~/.claude/hooks/quality-gates-v2.sh` | UPDATE | P1 |
| `~/.claude/schemas/task-classification-v2.yaml` | NEW | P0 |
| `~/.claude/schemas/plan-state-v2.json` | UPDATE | P1 |
| `~/.claude/hooks/recursive-decompose.sh` | NEW | P2 |
| Project `CLAUDE.md` | Update version, flow | P0 |
